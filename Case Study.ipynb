{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f6607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import xlrd\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7afdf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='./data/'\n",
    "DSS1 = np.loadtxt(data_path + 'DSS1.txt')\n",
    "DSS2 = np.loadtxt(data_path + 'DSS2.txt')\n",
    "#Integrated semantic similarity for diseases\n",
    "DSS = (DSS1 + DSS2) / 2\n",
    "#Gaussian interaction profile kernel similarity for diseases\n",
    "DGS = np.loadtxt(data_path + 'DGS.txt')\n",
    "#Integrated similarity for diseases\n",
    "IDS = np.zeros(shape = (DSS.shape[0], DSS.shape[1]))\n",
    "for i in range(DSS.shape[0]):\n",
    "    for j in range(DSS.shape[1]):\n",
    "        if DSS[i][j] == 0:\n",
    "            IDS[i][j] = DGS[i][j]\n",
    "        else:       \n",
    "            IDS[i][j] = DSS[i][j]\n",
    "#Functional similarity for miRNAs          \n",
    "MFS = np.loadtxt(data_path + 'MFS.txt')\n",
    "#Gaussian interaction profile kernel similarity for miRNAs\n",
    "MGS = np.loadtxt(data_path + 'MGS.txt')\n",
    "#Integrated similarity for miRNAs\n",
    "IMS = np.zeros(shape = (MFS.shape[0], MFS.shape[1]))\n",
    "for i in range(MFS.shape[0]):\n",
    "    for j in range(MFS.shape[1]):\n",
    "        if MFS[i][j] == 0:\n",
    "            IMS[i][j] = MGS[i][j]\n",
    "        else:\n",
    "            IMS[i][j] = MFS[i][j]\n",
    "#miRNA-disease associations matrix\n",
    "MD = np.zeros(shape = (DSS.shape[0], MFS.shape[0]))\n",
    "asso_file =  xlrd.open_workbook(data_path + 'Human miRNA-disease associations.xlsx')\n",
    "asso_pairs = asso_file.sheets()[0]\n",
    "for i in range(asso_pairs.nrows):\n",
    "    asso = asso_pairs.row_values(i)\n",
    "    m = int(asso[0])\n",
    "    n = int(asso[1])\n",
    "    MD[n-1,m-1]=1\n",
    "#Verified miRNA disease pair\n",
    "known=[]\n",
    "#Unverified miRNA disease pair\n",
    "unknown=[]\n",
    "for x in range(MD.shape[0]):\n",
    "    for y in range(MD.shape[1]):\n",
    "        if MD[x,y]==0:\n",
    "            unknown.append((x,y))\n",
    "        else:\n",
    "            known.append((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7368304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#position sample set\n",
    "posi_list = []\n",
    "#unlabeled sample set\n",
    "unlabelled_list = []\n",
    "#total sample set\n",
    "all_list = []\n",
    "\n",
    "for i in range(len(known)):\n",
    "    posi=IDS[known[i][0],:].tolist() + IMS[known[i][1],:].tolist()\n",
    "    posi_list.append(posi)\n",
    "    all_list.append(posi)\n",
    "\n",
    "for i in range(len(unknown)):\n",
    "    unlabelled=IDS[unknown[i][0],:].tolist() + IMS[unknown[i][1],:].tolist()\n",
    "    unlabelled_list.append(unlabelled)\n",
    "    all_list.append(unlabelled)\n",
    "\n",
    "#The total sample set is disordered to avoid the influence of order on the clustering results\n",
    "random.shuffle(all_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d94e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement MiniBatchKMeans clustering algorithm ten times for total sample set\n",
    "\n",
    "#Record the number of times each sample is divided into a certain cluster\n",
    "sum = np.zeros(len(all_list),dtype=int)\n",
    "#Record the final cluster of each sample\n",
    "final = np.zeros(len(all_list),dtype=int)\n",
    "for i in range(10):\n",
    "    #According to our experiment, we set the number of clusters to 2\n",
    "    cls = MiniBatchKMeans(n_clusters=2,batch_size=3072).fit(all_list)\n",
    "    yhat = cls.predict(all_list)\n",
    "    #According to the multiple clustering results of the total sample set, the total sample set is divided into subsets A and B\n",
    "    #Among the two subsets A and B, subset A (B) is always greater than the size of subset A (B)\n",
    "    #In order to ensure that subsets a (b) can obtain the same label after each clustering: \n",
    "    #suppose that the label of small subset is 1 and the label of large subset is 0\n",
    "    #After clustering, when the number of samples with label 1 is greater than the number of samples with label 0, \n",
    "    #the labels of subsets A and B are exchanged\n",
    "    if len(yhat[yhat==1]) > len(yhat[yhat==0]):\n",
    "        trans = yhat==0\n",
    "        yhat[yhat==1] = 0\n",
    "        yhat[trans] = 1\n",
    "    sum = sum + yhat\n",
    "    \n",
    "#When the number of times a sample gets a label of 1 is greater than or equal to 9, \n",
    "#it is considered that the label of this sample is 1\n",
    "final[sum<9] = 0\n",
    "final[sum>=9] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ea4ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organize the clustering results\n",
    "clusters = np.unique(final)\n",
    "subsets={}\n",
    "for i in clusters:\n",
    "    subset=[]\n",
    "    for j in range(len(all_list)):\n",
    "        if final[j] == i:\n",
    "            subset.append(all_list[j])\n",
    "    subsets[i]=subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ef743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store positive sample index in each subset\n",
    "index_lists=[]\n",
    "#Store the number of positive samples in each subset\n",
    "posi_cnt =[] \n",
    "\n",
    "for i in clusters:\n",
    "    index_list=[]\n",
    "    cnt=0\n",
    "    for j in range(len(subsets[i])):\n",
    "        if posi_list.__contains__(subsets[i][j]):\n",
    "            cnt=cnt+1\n",
    "            index_list.append(j)\n",
    "    index_lists.append(index_list)\n",
    "    posi_cnt.append(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd357c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of the total sample set is 189585, of which the number of positive samples is 5430\n",
      "The total number of samples in subset0 is 121411, of which the number of positive samples is 895, and the proportion of positive samples is 0.007372\n",
      "The total number of samples in subset1 is 68174, of which the number of positive samples is 4535, and the proportion of positive samples is 0.066521\n",
      "Subset0 has the least proportion of positive samples, accounting for 0.007372\n"
     ]
    }
   ],
   "source": [
    "#Find the subset with the least proportion of positive samples\n",
    "min_per=1\n",
    "min_idx=0\n",
    "print('The number of the total sample set is %d, of which the number of positive samples is %d' %(len(all_list),len(posi_list)))\n",
    "for i in range(len(posi_cnt)):\n",
    "    t_per=posi_cnt[i]/len(subsets[i])\n",
    "    print('The total number of samples in subset%d is %d, of which the number of positive samples is %d, and the proportion of positive samples is %f' %(i,len(subsets[i]),posi_cnt[i],t_per))\n",
    "    if t_per < min_per:\n",
    "        min_per=t_per\n",
    "        min_idx=i\n",
    "print('Subset%d has the least proportion of positive samples, accounting for %f' %(min_idx,min_per))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d10283bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the positive samples from the subset with the least number of positive samples, \n",
    "#and the remaining unmarked samples in the subset are regarded as negative samples\n",
    "new_nega=np.delete(subsets[min_idx], index_lists[min_idx], axis=0)\n",
    "new_nega=new_nega.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aabb06aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_xgb_learners(base_learner_num):\n",
    "    clfs = []\n",
    "    for i in range(base_learner_num):\n",
    "        clfs.append(XGBClassifier(max_depth=6,learning_rate=0.4,n_estimators=100))\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ace01a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection based on random forest feature importance score\n",
    "def feature_ranking_by_rf(data, label, sel_fea_num, sel_hp1, sel_hp2):\n",
    "    fs_rf = RandomForestClassifier(n_estimators=sel_hp1, max_depth=sel_hp2, random_state=0)\n",
    "    #Training random forest model\n",
    "    fs_rf.fit(data, label)\n",
    "    importances = fs_rf.feature_importances_\n",
    "    #Sort all features in the reverse order of feature importance scores, and return the sorted index value\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    #Extract the top sel_fea_num features with the highest feature importance score\n",
    "    most_imp = indices[:sel_fea_num]\n",
    "    return most_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1c79812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 1 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:14:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 2 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:14:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 3 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:14:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 4 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:15:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 5 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:15:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 6 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:15:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 7 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:16:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 8 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:16:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 9 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:16:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 10 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:17:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "sel_fea = 'RF'\n",
    "prop = 0.75\n",
    "base_learner_num = 10\n",
    "sel_fea_num = int(prop*len(posi_list[0]))\n",
    "sel_hp1 = 300\n",
    "sel_hp2 = 30\n",
    "\n",
    "posi_data=posi_list\n",
    "nega_data=new_nega\n",
    "posi_num = len(posi_data)\n",
    "\n",
    "base_learners = base_xgb_learners(base_learner_num)\n",
    "trained_clfs = []\n",
    "most_imps_list = []\n",
    "\n",
    "posi_train_data=np.array(posi_data)\n",
    "for i in range(base_learner_num):\n",
    "    print('the', i+1, 'th individual learner training\\n')\n",
    "    samples = random.sample(nega_data, posi_num)\n",
    "    nega_train_data=np.array(samples)\n",
    "    X_base_train = np.concatenate((posi_train_data, nega_train_data))\n",
    "    y_base_train = np.concatenate((np.ones(posi_train_data.shape[0]), np.zeros(nega_train_data.shape[0])))\n",
    "    if sel_fea == 'RF':\n",
    "        print('Feature selection in progress\\n')\n",
    "        most_imps = feature_ranking_by_rf(X_base_train, y_base_train, sel_fea_num, sel_hp1, sel_hp2)\n",
    "        most_imps_list.append(most_imps)\n",
    "        X_base_train = X_base_train[:,most_imps]\n",
    "    clf = base_learners[i]\n",
    "    clf.fit(X_base_train,y_base_train)\n",
    "    trained_clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "040de5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "miRNA_file = xlrd.open_workbook(data_path + 'miRNA number.xlsx').sheets()[0]\n",
    "miRNA = {}\n",
    "for i in range(miRNA_file.nrows):\n",
    "    miRNA[i] = miRNA_file.row_values(i)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27b4d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_file = xlrd.open_workbook(data_path + 'disease number.xlsx').sheets()[0]\n",
    "disease = {}\n",
    "for i in range(disease_file.nrows):\n",
    "    disease[disease_file.row_values(i)[1]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee8d518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 miRNAs most likely to be associated with Breast Neoplasms predicted by CSMDA are:\n",
      "hsa-mir-146a\thsa-mir-92a\thsa-mir-29b\thsa-mir-484\thsa-mir-127\t\n",
      "hsa-mir-505\thsa-mir-133a\thsa-mir-148b\thsa-mir-424\thsa-mir-128\t\n",
      "hsa-mir-1290\thsa-mir-139\thsa-mir-181b\thsa-mir-223\thsa-mir-1972\t\n",
      "hsa-mir-221\thsa-mir-16\thsa-let-7a\thsa-mir-29a\thsa-mir-1273a\t\n",
      "\n",
      "The top 20 miRNAs most likely to be associated with Colon Neoplasms predicted by CSMDA are:\n",
      "hsa-mir-1\thsa-mir-18b\thsa-let-7e\thsa-mir-1246\thsa-mir-483\t\n",
      "hsa-mir-21\thsa-mir-221\thsa-mir-106a\thsa-mir-152\thsa-mir-198\t\n",
      "hsa-mir-124\thsa-mir-18a\thsa-mir-141\thsa-mir-338\thsa-mir-16\t\n",
      "hsa-mir-28\thsa-mir-151a\thsa-mir-345\thsa-mir-29a\thsa-mir-210\t\n",
      "\n",
      "The top 20 miRNAs most likely to be associated with Lung Neoplasms predicted by CSMDA are:\n",
      "hsa-mir-183\thsa-let-7b\thsa-mir-34c\thsa-mir-150\thsa-mir-486\t\n",
      "hsa-mir-203\thsa-mir-223\thsa-mir-424\thsa-mir-382\thsa-mir-101\t\n",
      "hsa-mir-132\thsa-mir-196b\thsa-mir-340\thsa-mir-92a\thsa-mir-18a\t\n",
      "hsa-mir-29a\thsa-mir-137\thsa-mir-362\thsa-mir-191\thsa-mir-195\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cancer_list = ['Breast Neoplasms', 'Colon Neoplasms', 'Lung Neoplasms']\n",
    "\n",
    "miRNA_num = len(miRNA)\n",
    "for cancer in cancer_list:\n",
    "    cancer_index = disease[cancer]\n",
    "    test_sample_list = []\n",
    "    for i in range(miRNA_num):\n",
    "        test_sample = IDS[cancer_index,:].tolist() + IMS[i,:].tolist()\n",
    "        test_sample_list.append(test_sample)\n",
    "    test_sample_list = np.array(test_sample_list)\n",
    "    #The prediction score of each individual learner on the prediction sample\n",
    "    prob_list = []\n",
    "    for i,clf in enumerate(trained_clfs):\n",
    "        most_imp = most_imps_list[i]\n",
    "        test_data = test_sample_list[:,most_imp]\n",
    "        prob = clf.predict_proba(test_data)\n",
    "        prob_list.append(prob[:,1])\n",
    "    prob_list = np.array(prob_list)\n",
    "    base_probs = np.transpose(prob_list)\n",
    "    #Soft voting strategy\n",
    "    pred_final = []\n",
    "    prob_final = []\n",
    "    for prob in base_probs:\n",
    "        mean_prob = np.mean(prob)\n",
    "        prob_final.append(mean_prob)\n",
    "        if mean_prob > 0.5:\n",
    "            pred_final.append(1)\n",
    "        else:\n",
    "            pred_final.append(0)\n",
    "    pred_final = np.array(pred_final)\n",
    "    prob_final = np.array(prob_final)\n",
    "    prob_final = prob_final[pred_final == 1]\n",
    "    sort_index = np.argsort(prob_final)[::-1]\n",
    "    pred_miRNA = []\n",
    "    for i in sort_index:\n",
    "        pred_miRNA.append(miRNA[i])\n",
    "    result = 'The top 20 miRNAs most likely to be associated with ' + cancer + ' predicted by CSMDA are:\\n'\n",
    "    cnt = 1\n",
    "    for i in range(20):\n",
    "        result = result + pred_miRNA[i] + '\\t'\n",
    "        if cnt % 5 == 0:\n",
    "            result = result + '\\n'\n",
    "        cnt += 1\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
