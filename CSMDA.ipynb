{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891886a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import xlrd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, roc_curve, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402fe09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save or read variables\n",
    "import pickle\n",
    "def save_variable(v,filename):\n",
    "    f=open(filename,'wb')\n",
    "    pickle.dump(v,f)\n",
    "    f.close()\n",
    "    return filename\n",
    "  \n",
    "def load_variavle(filename):\n",
    "    f=open(filename,'rb')\n",
    "    r=pickle.load(f)\n",
    "    f.close()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da80d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='./data/'\n",
    "DSS1 = np.loadtxt(data_path + 'DSS1.txt')\n",
    "DSS2 = np.loadtxt(data_path + 'DSS2.txt')\n",
    "#Integrated semantic similarity for diseases\n",
    "DSS = (DSS1 + DSS2) / 2\n",
    "#Gaussian interaction profile kernel similarity for diseases\n",
    "DGS = np.loadtxt(data_path + 'DGS.txt')\n",
    "#Integrated similarity for diseases\n",
    "IDS = np.zeros(shape = (DSS.shape[0], DSS.shape[1]))\n",
    "for i in range(DSS.shape[0]):\n",
    "    for j in range(DSS.shape[1]):\n",
    "        if DSS[i][j] == 0:\n",
    "            IDS[i][j] = DGS[i][j]\n",
    "        else:       \n",
    "            IDS[i][j] = DSS[i][j]\n",
    "#Functional similarity for miRNAs          \n",
    "MFS = np.loadtxt(data_path + 'MFS.txt')\n",
    "#Gaussian interaction profile kernel similarity for miRNAs\n",
    "MGS = np.loadtxt(data_path + 'MGS.txt')\n",
    "#Integrated similarity for miRNAs\n",
    "IMS = np.zeros(shape = (MFS.shape[0], MFS.shape[1]))\n",
    "for i in range(MFS.shape[0]):\n",
    "    for j in range(MFS.shape[1]):\n",
    "        if MFS[i][j] == 0:\n",
    "            IMS[i][j] = MGS[i][j]\n",
    "        else:\n",
    "            IMS[i][j] = MFS[i][j]\n",
    "#miRNA-disease associations matrix\n",
    "MD = np.zeros(shape = (DSS.shape[0], MFS.shape[0]))\n",
    "asso_file =  xlrd.open_workbook(data_path + 'Human miRNA-disease associations.xlsx')\n",
    "asso_pairs = asso_file.sheets()[0]\n",
    "for i in range(asso_pairs.nrows):\n",
    "    asso = asso_pairs.row_values(i)\n",
    "    m = int(asso[0])\n",
    "    n = int(asso[1])\n",
    "    MD[n-1,m-1]=1\n",
    "#Verified miRNA disease pair\n",
    "known=[]\n",
    "#Unverified miRNA disease pair\n",
    "unknown=[]\n",
    "for x in range(MD.shape[0]):\n",
    "    for y in range(MD.shape[1]):\n",
    "        if MD[x,y]==0:\n",
    "            unknown.append((x,y))\n",
    "        else:\n",
    "            known.append((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d69e9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#position sample set\n",
    "posi_list = []\n",
    "#unlabeled sample set\n",
    "unlabelled_list = []\n",
    "#total sample set\n",
    "all_list = []\n",
    "\n",
    "for i in range(len(known)):\n",
    "    posi=IDS[known[i][0],:].tolist() + IMS[known[i][1],:].tolist()\n",
    "    posi_list.append(posi)\n",
    "    all_list.append(posi)\n",
    "\n",
    "for i in range(len(unknown)):\n",
    "    unlabelled=IDS[unknown[i][0],:].tolist() + IMS[unknown[i][1],:].tolist()\n",
    "    unlabelled_list.append(unlabelled)\n",
    "    all_list.append(unlabelled)\n",
    "\n",
    "#The total sample set is disordered to avoid the influence of order on the clustering results\n",
    "random.shuffle(all_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1cc007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement MiniBatchKMeans clustering algorithm ten times for total sample set\n",
    "\n",
    "#Record the number of times each sample is divided into a certain cluster\n",
    "sum = np.zeros(len(all_list),dtype=int)\n",
    "#Record the final cluster of each sample\n",
    "final = np.zeros(len(all_list),dtype=int)\n",
    "for i in range(10):\n",
    "    #According to our experiment, we set the number of clusters to 2\n",
    "    cls = MiniBatchKMeans(n_clusters=2,batch_size=3072).fit(all_list)\n",
    "    yhat = cls.predict(all_list)\n",
    "    #According to the multiple clustering results of the total sample set, the total sample set is divided into subsets A and B\n",
    "    #Among the two subsets A and B, subset A (B) is always greater than the size of subset A (B)\n",
    "    #In order to ensure that subsets a (b) can obtain the same label after each clustering: \n",
    "    #suppose that the label of small subset is 1 and the label of large subset is 0\n",
    "    #After clustering, when the number of samples with label 1 is greater than the number of samples with label 0, \n",
    "    #the labels of subsets A and B are exchanged\n",
    "    if len(yhat[yhat==1]) > len(yhat[yhat==0]):\n",
    "        trans = yhat==0\n",
    "        yhat[yhat==1] = 0\n",
    "        yhat[trans] = 1\n",
    "    sum = sum + yhat\n",
    "    \n",
    "#When the number of times a sample gets a label of 1 is greater than or equal to 9, \n",
    "#it is considered that the label of this sample is 1\n",
    "final[sum<9] = 0\n",
    "final[sum>=9] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ebe21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organize the clustering results\n",
    "clusters = np.unique(final)\n",
    "subsets={}\n",
    "for i in clusters:\n",
    "    subset=[]\n",
    "    for j in range(len(all_list)):\n",
    "        if final[j] == i:\n",
    "            subset.append(all_list[j])\n",
    "    subsets[i]=subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c52d3b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store positive sample index in each subset\n",
    "index_lists=[]\n",
    "#Store the number of positive samples in each subset\n",
    "posi_cnt =[] \n",
    "\n",
    "for i in clusters:\n",
    "    index_list=[]\n",
    "    cnt=0\n",
    "    for j in range(len(subsets[i])):\n",
    "        if posi_list.__contains__(subsets[i][j]):\n",
    "            cnt=cnt+1\n",
    "            index_list.append(j)\n",
    "    index_lists.append(index_list)\n",
    "    posi_cnt.append(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a1b9ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of the total sample set is 189585, of which the number of positive samples is 5430\n",
      "The total number of samples in subset0 is 121411, of which the number of positive samples is 895, and the proportion of positive samples is 0.007372\n",
      "The total number of samples in subset1 is 68174, of which the number of positive samples is 4535, and the proportion of positive samples is 0.066521\n",
      "Subset0 has the least proportion of positive samples, accounting for 0.007372\n"
     ]
    }
   ],
   "source": [
    "#Find the subset with the least proportion of positive samples\n",
    "min_per=1\n",
    "min_idx=0\n",
    "print('The number of the total sample set is %d, of which the number of positive samples is %d' %(len(all_list),len(posi_list)))\n",
    "for i in range(len(posi_cnt)):\n",
    "    t_per=posi_cnt[i]/len(subsets[i])\n",
    "    print('The total number of samples in subset%d is %d, of which the number of positive samples is %d, and the proportion of positive samples is %f' %(i,len(subsets[i]),posi_cnt[i],t_per))\n",
    "    if t_per < min_per:\n",
    "        min_per=t_per\n",
    "        min_idx=i\n",
    "print('Subset%d has the least proportion of positive samples, accounting for %f' %(min_idx,min_per))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67eac713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the positive samples from the subset with the least number of positive samples, \n",
    "#and the remaining unmarked samples in the subset are regarded as negative samples\n",
    "new_nega=np.delete(subsets[min_idx], index_lists[min_idx], axis=0)\n",
    "new_nega=new_nega.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "682bb1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The negative sample set was divided into two parts according to positive_num\n",
    "def spliting_negative_data(negative_data, positive_num):\n",
    "    negative_train_data = negative_data[positive_num:]\n",
    "    negative_cv_data = negative_data[:positive_num]\n",
    "    return negative_train_data, negative_cv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48828b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection based on random forest feature importance score\n",
    "def feature_ranking_by_rf(data, label, sel_fea_num, sel_hp1, sel_hp2):\n",
    "    fs_rf = RandomForestClassifier(n_estimators=sel_hp1, max_depth=sel_hp2, random_state=0)\n",
    "    #Training random forest model\n",
    "    fs_rf.fit(data, label)\n",
    "    importances = fs_rf.feature_importances_\n",
    "    #Sort all features in the reverse order of feature importance scores, and return the sorted index value\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    #Extract the top sel_fea_num features with the highest feature importance score\n",
    "    most_imp = indices[:sel_fea_num]\n",
    "    return most_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab1a69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The prediction score of each individual learner to the samples in the test set after applying the feature selection method\n",
    "def base_probs_fs(X_test, most_imps_list, trained_clfs):\n",
    "    prob_list = []\n",
    "    for i,clf in enumerate(trained_clfs):\n",
    "        #Index value of key features\n",
    "        most_imp = most_imps_list[i]\n",
    "        #Extract key features from test set\n",
    "        X_test_fs = X_test[:,most_imp]\n",
    "        #Prediction score of individual learner on test set\n",
    "        prob = clf.predict_proba(X_test_fs)\n",
    "        prob_list.append(prob[:,1])\n",
    "    prob_list = np.array(prob_list)\n",
    "    base_probs = np.transpose(prob_list)\n",
    "    return base_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ae450b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The prediction score of each individual learner to the samples in the test set without applying the feature selection method\n",
    "def base_probs(X_test, trained_clfs):\n",
    "    prob_list = []\n",
    "    for clf in trained_clfs:\n",
    "        prob = clf.predict_proba(X_test)\n",
    "        prob_list.append(prob[:,1])\n",
    "    prob_list = np.array(prob_list)\n",
    "    base_probs = np.transpose(prob_list)\n",
    "    return base_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "88b3c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost as individual learners\n",
    "def base_xgb_learners(base_learner_num):\n",
    "    clfs = []\n",
    "    for i in range(base_learner_num):\n",
    "        clfs.append(XGBClassifier(max_depth=6,learning_rate=0.4,n_estimators=100))\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3e1c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest as individual learners\n",
    "def base_rf_learners(base_learner_num):\n",
    "    clfs = []\n",
    "    for i in range(base_learner_num):\n",
    "        clfs.append(RandomForestClassifier(n_estimators=400, max_depth=40))\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c37a1005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExtraTrees as individual learners\n",
    "def base_ert_learners(base_learner_num):\n",
    "    clfs = []\n",
    "    for i in range(base_learner_num):\n",
    "        clfs.append(ExtraTreesClassifier(n_estimators=400, max_depth=20))\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae178934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost as individual learners\n",
    "def base_ab_learners(base_learner_num):\n",
    "    clfs = []\n",
    "    for i in range(base_learner_num):\n",
    "        clfs.append(AdaBoostClassifier(n_estimators=50, learning_rate=0.35))\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a098e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#soft voting strategy\n",
    "def soft_voting_strategy(base_probs):\n",
    "    print('\\nSoft voting ...\\n')\n",
    "    #Store the final prediction score of each sample\n",
    "    pred_final = []\n",
    "    #Store the final prediction result of each sample\n",
    "    prob_final = []\n",
    "    for prob in base_probs:\n",
    "        #Calculate the average of all individual learner prediction scores\n",
    "        mean_prob = np.mean(prob)\n",
    "        prob_final.append(mean_prob)\n",
    "        if mean_prob > 0.5:\n",
    "            pred_final.append(1)\n",
    "        else:\n",
    "            pred_final.append(0)\n",
    "    return pred_final, prob_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8216518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance calculation\n",
    "def calculate_performace(num, y_pred, y_prob, y_test):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for index in range(num):\n",
    "        if y_test[index] ==1:\n",
    "            if y_test[index] == y_pred[index]:\n",
    "                tp = tp + 1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if y_test[index] == y_pred[index]:\n",
    "                tn = tn + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "    acc = float(tp + tn)/num\n",
    "    try:\n",
    "        precision = float(tp)/(tp + fp)\n",
    "        recall = float(tp)/ (tp + fn)\n",
    "        f1_score = float((2*precision*recall)/(precision+recall))\n",
    "    except ZeroDivisionError:\n",
    "        print(\"You can't divide by 0.\")\n",
    "        precision=recall=f1_score = 100\n",
    "    AUC = roc_auc_score(y_test, y_prob)\n",
    "    p, r, _ = precision_recall_curve(y_test,y_prob)\n",
    "    AUPR = auc(r, p)\n",
    "    return tp, fp, tn, fn, acc, precision, recall, f1_score, AUC, AUPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8cb3bd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 cross validation\n",
      "\n",
      "the 1 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 1 th individual learner proformance: \n",
      "  Acc = \t 0.962707182320442 \n",
      "  prec = \t 0.96398891966759 \n",
      "  recall = \t 0.9613259668508287 \n",
      "  f1_score = \t 0.9626556016597511 \n",
      "  AUC = \t 0.992465634952128 \n",
      "  aupr = \t 0.9937499946378503 \n",
      "\n",
      "Round 1 cross validation\n",
      "\n",
      "the 2 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 2 th individual learner proformance: \n",
      "  Acc = \t 0.9636279926335175 \n",
      "  prec = \t 0.9649122807017544 \n",
      "  recall = \t 0.9622467771639043 \n",
      "  f1_score = \t 0.9635776855693868 \n",
      "  AUC = \t 0.9921586981811029 \n",
      "  aupr = \t 0.9935656953271996 \n",
      "\n",
      "Round 1 cross validation\n",
      "\n",
      "the 3 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 3 th individual learner proformance: \n",
      "  Acc = \t 0.9585635359116023 \n",
      "  prec = \t 0.9628252788104089 \n",
      "  recall = \t 0.9539594843462247 \n",
      "  f1_score = \t 0.9583718778908417 \n",
      "  AUC = \t 0.9928505777533585 \n",
      "  aupr = \t 0.9938855172941166 \n",
      "\n",
      "Round 1 cross validation\n",
      "\n",
      "the 4 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 4 th individual learner proformance: \n",
      "  Acc = \t 0.9636279926335175 \n",
      "  prec = \t 0.9632014719411224 \n",
      "  recall = \t 0.9640883977900553 \n",
      "  f1_score = \t 0.9636447307869305 \n",
      "  AUC = \t 0.9922112674623281 \n",
      "  aupr = \t 0.9936562222184856 \n",
      "\n",
      "Round 1 cross validation\n",
      "\n",
      "the 5 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:35:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 5 th individual learner proformance: \n",
      "  Acc = \t 0.9594843462246777 \n",
      "  prec = \t 0.956959706959707 \n",
      "  recall = \t 0.9622467771639043 \n",
      "  f1_score = \t 0.9595959595959596 \n",
      "  AUC = \t 0.9926911741264173 \n",
      "  aupr = \t 0.9938500599037708 \n",
      "\n",
      "Round 1 cross validation\n",
      "\n",
      "the 6 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:35:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 6 th individual learner proformance: \n",
      "  Acc = \t 0.9590239410681399 \n",
      "  prec = \t 0.9628597957288765 \n",
      "  recall = \t 0.9548802946593001 \n",
      "  f1_score = \t 0.9588534442903375 \n",
      "  AUC = \t 0.9924240882621272 \n",
      "  aupr = \t 0.9936769987919324 \n",
      "\n",
      "Round 1 cross validation\n",
      "\n",
      "the 7 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:35:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 7 th individual learner proformance: \n",
      "  Acc = \t 0.9594843462246777 \n",
      "  prec = \t 0.9594843462246777 \n",
      "  recall = \t 0.9594843462246777 \n",
      "  f1_score = \t 0.9594843462246777 \n",
      "  AUC = \t 0.9924469813362093 \n",
      "  aupr = \t 0.9936709111584046 \n",
      "\n",
      "Round 1 cross validation\n",
      "\n",
      "the 8 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:36:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 8 th individual learner proformance: \n",
      "  Acc = \t 0.9567219152854513 \n",
      "  prec = \t 0.9542124542124543 \n",
      "  recall = \t 0.9594843462246777 \n",
      "  f1_score = \t 0.9568411386593205 \n",
      "  AUC = \t 0.9924147614541682 \n",
      "  aupr = \t 0.9936040024296637 \n",
      "\n",
      "Round 1 cross validation\n",
      "\n",
      "the 9 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:36:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 9 th individual learner proformance: \n",
      "  Acc = \t 0.9567219152854513 \n",
      "  prec = \t 0.9618249534450651 \n",
      "  recall = \t 0.9511970534069981 \n",
      "  f1_score = \t 0.9564814814814815 \n",
      "  AUC = \t 0.9923494737984527 \n",
      "  aupr = \t 0.9934666255376695 \n",
      "\n",
      "Round 1 cross validation\n",
      "\n",
      "the 10 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:36:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 10 th individual learner proformance: \n",
      "  Acc = \t 0.9576427255985267 \n",
      "  prec = \t 0.9568014705882353 \n",
      "  recall = \t 0.9585635359116023 \n",
      "  f1_score = \t 0.9576816927322906 \n",
      "  AUC = \t 0.9925436409823333 \n",
      "  aupr = \t 0.9935363965551359 \n",
      "\n",
      "\n",
      "Soft voting ...\n",
      "\n",
      "Round  0 cross validation proformance after soft voting: \n",
      "  Acc = \t 0.9622467771639043 \n",
      "  prec = \t 0.966542750929368 \n",
      "  recall = \t 0.9576427255985267 \n",
      "  f1_score = \t 0.9620721554116558 \n",
      "  AUC = \t 0.9936687931788815 \n",
      "  AUPR = \t 0.9947509196520367 \n",
      "\n",
      "Round 2 cross validation\n",
      "\n",
      "the 1 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:36:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 1 th individual learner proformance: \n",
      "  Acc = \t 0.9548802946593001 \n",
      "  prec = \t 0.960820895522388 \n",
      "  recall = \t 0.9484346224677717 \n",
      "  f1_score = \t 0.9545875810936052 \n",
      "  AUC = \t 0.993108336809689 \n",
      "  aupr = \t 0.9939129841899232 \n",
      "\n",
      "Round 2 cross validation\n",
      "\n",
      "the 2 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:37:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 2 th individual learner proformance: \n",
      "  Acc = \t 0.9599447513812155 \n",
      "  prec = \t 0.9620721554116559 \n",
      "  recall = \t 0.9576427255985267 \n",
      "  f1_score = \t 0.9598523304107062 \n",
      "  AUC = \t 0.9936357254052075 \n",
      "  aupr = \t 0.9945548864989189 \n",
      "\n",
      "Round 2 cross validation\n",
      "\n",
      "the 3 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:37:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 3 th individual learner proformance: \n",
      "  Acc = \t 0.9608655616942909 \n",
      "  prec = \t 0.9699530516431925 \n",
      "  recall = \t 0.9511970534069981 \n",
      "  f1_score = \t 0.9604834960483495 \n",
      "  AUC = \t 0.9926606500276414 \n",
      "  aupr = \t 0.993765299582878 \n",
      "\n",
      "Round 2 cross validation\n",
      "\n",
      "the 4 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:37:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 4 th individual learner proformance: \n",
      "  Acc = \t 0.9562615101289135 \n",
      "  prec = \t 0.9643861293345829 \n",
      "  recall = \t 0.9475138121546961 \n",
      "  f1_score = \t 0.9558755225267068 \n",
      "  AUC = \t 0.9928514256449911 \n",
      "  aupr = \t 0.9938568842417442 \n",
      "\n",
      "Round 2 cross validation\n",
      "\n",
      "the 5 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:37:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 5 th individual learner proformance: \n",
      "  Acc = \t 0.9585635359116023 \n",
      "  prec = \t 0.9671669793621013 \n",
      "  recall = \t 0.9493554327808471 \n",
      "  f1_score = \t 0.95817843866171 \n",
      "  AUC = \t 0.9929904798727484 \n",
      "  aupr = \t 0.9939370775938431 \n",
      "\n",
      "Round 2 cross validation\n",
      "\n",
      "the 6 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:38:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 6 th individual learner proformance: \n",
      "  Acc = \t 0.9548802946593001 \n",
      "  prec = \t 0.9582560296846011 \n",
      "  recall = \t 0.9511970534069981 \n",
      "  f1_score = \t 0.9547134935304991 \n",
      "  AUC = \t 0.9927344165996832 \n",
      "  aupr = \t 0.993607851422988 \n",
      "\n",
      "Round 2 cross validation\n",
      "\n",
      "the 7 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:38:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 7 th individual learner proformance: \n",
      "  Acc = \t 0.9594843462246777 \n",
      "  prec = \t 0.9689849624060151 \n",
      "  recall = \t 0.9493554327808471 \n",
      "  f1_score = \t 0.9590697674418606 \n",
      "  AUC = \t 0.9931176636176483 \n",
      "  aupr = \t 0.9941074320674828 \n",
      "\n",
      "Round 2 cross validation\n",
      "\n",
      "the 8 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:38:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 8 th individual learner proformance: \n",
      "  Acc = \t 0.9571823204419889 \n",
      "  prec = \t 0.9653233364573571 \n",
      "  recall = \t 0.9484346224677717 \n",
      "  f1_score = \t 0.9568044588945657 \n",
      "  AUC = \t 0.9939519889841919 \n",
      "  aupr = \t 0.994724849394144 \n",
      "\n",
      "Round 2 cross validation\n",
      "\n",
      "the 9 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:39:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 9 th individual learner proformance: \n",
      "  Acc = \t 0.9567219152854513 \n",
      "  prec = \t 0.9652908067542214 \n",
      "  recall = \t 0.9475138121546961 \n",
      "  f1_score = \t 0.9563197026022305 \n",
      "  AUC = \t 0.9929124738425431 \n",
      "  aupr = \t 0.9938998981402061 \n",
      "\n",
      "Round 2 cross validation\n",
      "\n",
      "the 10 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:39:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 10 th individual learner proformance: \n",
      "  Acc = \t 0.9604051565377533 \n",
      "  prec = \t 0.9672897196261683 \n",
      "  recall = \t 0.9530386740331491 \n",
      "  f1_score = \t 0.9601113172541744 \n",
      "  AUC = \t 0.993791737465618 \n",
      "  aupr = \t 0.9945870219627587 \n",
      "\n",
      "\n",
      "Soft voting ...\n",
      "\n",
      "Round  1 cross validation proformance after soft voting: \n",
      "  Acc = \t 0.9604051565377533 \n",
      "  prec = \t 0.9699248120300752 \n",
      "  recall = \t 0.9502762430939227 \n",
      "  f1_score = \t 0.96 \n",
      "  AUC = \t 0.994200421232563 \n",
      "  AUPR = \t 0.9949623582920759 \n",
      "\n",
      "Round 3 cross validation\n",
      "\n",
      "the 1 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:39:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 1 th individual learner proformance: \n",
      "  Acc = \t 0.9576427255985267 \n",
      "  prec = \t 0.9644859813084112 \n",
      "  recall = \t 0.9502762430939227 \n",
      "  f1_score = \t 0.9573283858998145 \n",
      "  AUC = \t 0.9913362432974164 \n",
      "  aupr = \t 0.9927581812738054 \n",
      "\n",
      "Round 3 cross validation\n",
      "\n",
      "the 2 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:39:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 2 th individual learner proformance: \n",
      "  Acc = \t 0.9558011049723757 \n",
      "  prec = \t 0.9558011049723757 \n",
      "  recall = \t 0.9558011049723757 \n",
      "  f1_score = \t 0.9558011049723757 \n",
      "  AUC = \t 0.9911666649708833 \n",
      "  aupr = \t 0.9925314722675324 \n",
      "\n",
      "Round 3 cross validation\n",
      "\n",
      "the 3 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:40:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 3 th individual learner proformance: \n",
      "  Acc = \t 0.9548802946593001 \n",
      "  prec = \t 0.955719557195572 \n",
      "  recall = \t 0.9539594843462247 \n",
      "  f1_score = \t 0.9548387096774194 \n",
      "  AUC = \t 0.9916770957337485 \n",
      "  aupr = \t 0.9929105720694453 \n",
      "\n",
      "Round 3 cross validation\n",
      "\n",
      "the 4 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:40:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 4 th individual learner proformance: \n",
      "  Acc = \t 0.9585635359116023 \n",
      "  prec = \t 0.9611111111111111 \n",
      "  recall = \t 0.9558011049723757 \n",
      "  f1_score = \t 0.9584487534626039 \n",
      "  AUC = \t 0.9911937975031289 \n",
      "  aupr = \t 0.992566843714421 \n",
      "\n",
      "Round 3 cross validation\n",
      "\n",
      "the 5 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:40:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 5 th individual learner proformance: \n",
      "  Acc = \t 0.9530386740331491 \n",
      "  prec = \t 0.9555555555555556 \n",
      "  recall = \t 0.9502762430939227 \n",
      "  f1_score = \t 0.9529085872576178 \n",
      "  AUC = \t 0.9906926935482231 \n",
      "  aupr = \t 0.9921220016352316 \n",
      "\n",
      "Round 3 cross validation\n",
      "\n",
      "the 6 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:41:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 6 th individual learner proformance: \n",
      "  Acc = \t 0.9521178637200737 \n",
      "  prec = \t 0.9537892791127541 \n",
      "  recall = \t 0.9502762430939227 \n",
      "  f1_score = \t 0.9520295202952029 \n",
      "  AUC = \t 0.9904205203341372 \n",
      "  aupr = \t 0.992078568769249 \n",
      "\n",
      "Round 3 cross validation\n",
      "\n",
      "the 7 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:41:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 7 th individual learner proformance: \n",
      "  Acc = \t 0.9553406998158379 \n",
      "  prec = \t 0.9532538955087076 \n",
      "  recall = \t 0.9576427255985267 \n",
      "  f1_score = \t 0.9554432705558109 \n",
      "  AUC = \t 0.9913820294455806 \n",
      "  aupr = \t 0.9928539159493097 \n",
      "\n",
      "Round 3 cross validation\n",
      "\n",
      "the 8 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:41:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 8 th individual learner proformance: \n",
      "  Acc = \t 0.9544198895027625 \n",
      "  prec = \t 0.9548387096774194 \n",
      "  recall = \t 0.9539594843462247 \n",
      "  f1_score = \t 0.954398894518655 \n",
      "  AUC = \t 0.9913226770312941 \n",
      "  aupr = \t 0.9928702998854557 \n",
      "\n",
      "Round 3 cross validation\n",
      "\n",
      "the 9 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:41:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 9 th individual learner proformance: \n",
      "  Acc = \t 0.9590239410681399 \n",
      "  prec = \t 0.9611470860314524 \n",
      "  recall = \t 0.9567219152854513 \n",
      "  f1_score = \t 0.9589293954776189 \n",
      "  AUC = \t 0.9913044473611916 \n",
      "  aupr = \t 0.9928011039946012 \n",
      "\n",
      "Round 3 cross validation\n",
      "\n",
      "the 10 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:42:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 10 th individual learner proformance: \n",
      "  Acc = \t 0.9558011049723757 \n",
      "  prec = \t 0.9634831460674157 \n",
      "  recall = \t 0.9475138121546961 \n",
      "  f1_score = \t 0.9554317548746517 \n",
      "  AUC = \t 0.9905375293794451 \n",
      "  aupr = \t 0.9920551426800286 \n",
      "\n",
      "\n",
      "Soft voting ...\n",
      "\n",
      "Round  2 cross validation proformance after soft voting: \n",
      "  Acc = \t 0.9599447513812155 \n",
      "  prec = \t 0.9637883008356546 \n",
      "  recall = \t 0.9558011049723757 \n",
      "  f1_score = \t 0.9597780859916782 \n",
      "  AUC = \t 0.9922672283100842 \n",
      "  AUPR = \t 0.993627838186911 \n",
      "\n",
      "Round 4 cross validation\n",
      "\n",
      "the 1 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:42:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 1 th individual learner proformance: \n",
      "  Acc = \t 0.9594843462246777 \n",
      "  prec = \t 0.970754716981132 \n",
      "  recall = \t 0.9475138121546961 \n",
      "  f1_score = \t 0.9589934762348555 \n",
      "  AUC = \t 0.9914549481259899 \n",
      "  aupr = \t 0.9926869661381762 \n",
      "\n",
      "Round 4 cross validation\n",
      "\n",
      "the 2 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:42:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 2 th individual learner proformance: \n",
      "  Acc = \t 0.9534990791896869 \n",
      "  prec = \t 0.9624413145539906 \n",
      "  recall = \t 0.9438305709023941 \n",
      "  f1_score = \t 0.9530450953045095 \n",
      "  AUC = \t 0.9903671031612792 \n",
      "  aupr = \t 0.9918504484303247 \n",
      "\n",
      "Round 4 cross validation\n",
      "\n",
      "the 3 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:42:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 3 th individual learner proformance: \n",
      "  Acc = \t 0.9488950276243094 \n",
      "  prec = \t 0.9577464788732394 \n",
      "  recall = \t 0.9392265193370166 \n",
      "  f1_score = \t 0.9483960948396094 \n",
      "  AUC = \t 0.9891630970428932 \n",
      "  aupr = \t 0.9909653289230775 \n",
      "\n",
      "Round 4 cross validation\n",
      "\n",
      "the 4 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:43:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 4 th individual learner proformance: \n",
      "  Acc = \t 0.9567219152854513 \n",
      "  prec = \t 0.9679245283018868 \n",
      "  recall = \t 0.9447513812154696 \n",
      "  f1_score = \t 0.9561975768872321 \n",
      "  AUC = \t 0.9906146875180176 \n",
      "  aupr = \t 0.9921122267070746 \n",
      "\n",
      "Round 4 cross validation\n",
      "\n",
      "the 5 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:43:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 5 th individual learner proformance: \n",
      "  Acc = \t 0.9562615101289135 \n",
      "  prec = \t 0.9661335841956726 \n",
      "  recall = \t 0.9456721915285451 \n",
      "  f1_score = \t 0.9557933922754769 \n",
      "  AUC = \t 0.9904069540680145 \n",
      "  aupr = \t 0.9920172777309104 \n",
      "\n",
      "Round 4 cross validation\n",
      "\n",
      "the 6 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:43:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 6 th individual learner proformance: \n",
      "  Acc = \t 0.9576427255985267 \n",
      "  prec = \t 0.967984934086629 \n",
      "  recall = \t 0.9465930018416207 \n",
      "  f1_score = \t 0.957169459962756 \n",
      "  AUC = \t 0.9902212658004605 \n",
      "  aupr = \t 0.9918791554702294 \n",
      "\n",
      "Round 4 cross validation\n",
      "\n",
      "the 7 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:43:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 7 th individual learner proformance: \n",
      "  Acc = \t 0.9525782688766115 \n",
      "  prec = \t 0.9606373008434864 \n",
      "  recall = \t 0.9438305709023941 \n",
      "  f1_score = \t 0.9521597770552717 \n",
      "  AUC = \t 0.9905968817937317 \n",
      "  aupr = \t 0.9918510634646125 \n",
      "\n",
      "Round 4 cross validation\n",
      "\n",
      "the 8 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:44:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 8 th individual learner proformance: \n",
      "  Acc = \t 0.9539594843462247 \n",
      "  prec = \t 0.9650943396226415 \n",
      "  recall = \t 0.9419889502762431 \n",
      "  f1_score = \t 0.9534016775396086 \n",
      "  AUC = \t 0.9904747853986278 \n",
      "  aupr = \t 0.9919305647601392 \n",
      "\n",
      "Round 4 cross validation\n",
      "\n",
      "the 9 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:44:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 9 th individual learner proformance: \n",
      "  Acc = \t 0.9507366482504604 \n",
      "  prec = \t 0.9631031220435194 \n",
      "  recall = \t 0.9373848987108656 \n",
      "  f1_score = \t 0.9500699953336444 \n",
      "  AUC = \t 0.9904985263643424 \n",
      "  aupr = \t 0.9918277126155538 \n",
      "\n",
      "Round 4 cross validation\n",
      "\n",
      "the 10 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:44:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 10 th individual learner proformance: \n",
      "  Acc = \t 0.9493554327808471 \n",
      "  prec = \t 0.9535315985130112 \n",
      "  recall = \t 0.9447513812154696 \n",
      "  f1_score = \t 0.9491211840888067 \n",
      "  AUC = \t 0.9903230127963806 \n",
      "  aupr = \t 0.9916587468476883 \n",
      "\n",
      "\n",
      "Soft voting ...\n",
      "\n",
      "Round  3 cross validation proformance after soft voting: \n",
      "  Acc = \t 0.9571823204419889 \n",
      "  prec = \t 0.9706161137440759 \n",
      "  recall = \t 0.9429097605893186 \n",
      "  f1_score = \t 0.9565623540401682 \n",
      "  AUC = \t 0.9917118592906878 \n",
      "  AUPR = \t 0.9929861274970534 \n",
      "\n",
      "Round 5 cross validation\n",
      "\n",
      "the 1 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:45:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 1 th individual learner proformance: \n",
      "  Acc = \t 0.9640883977900553 \n",
      "  prec = \t 0.9666666666666667 \n",
      "  recall = \t 0.9613259668508287 \n",
      "  f1_score = \t 0.9639889196675899 \n",
      "  AUC = \t 0.9922044843292669 \n",
      "  aupr = \t 0.9935378713014859 \n",
      "\n",
      "Round 5 cross validation\n",
      "\n",
      "the 2 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:45:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 2 th individual learner proformance: \n",
      "  Acc = \t 0.9604051565377533 \n",
      "  prec = \t 0.9655493482309124 \n",
      "  recall = \t 0.9548802946593001 \n",
      "  f1_score = \t 0.9601851851851851 \n",
      "  AUC = \t 0.9921459798066128 \n",
      "  aupr = \t 0.9936752353024809 \n",
      "\n",
      "Round 5 cross validation\n",
      "\n",
      "the 3 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:45:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 3 th individual learner proformance: \n",
      "  Acc = \t 0.9631675874769797 \n",
      "  prec = \t 0.9718574108818011 \n",
      "  recall = \t 0.9539594843462247 \n",
      "  f1_score = \t 0.9628252788104089 \n",
      "  AUC = \t 0.9912972402823139 \n",
      "  aupr = \t 0.9932500347669975 \n",
      "\n",
      "Round 5 cross validation\n",
      "\n",
      "the 4 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:45:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 4 th individual learner proformance: \n",
      "  Acc = \t 0.964548802946593 \n",
      "  prec = \t 0.9701770736253494 \n",
      "  recall = \t 0.9585635359116023 \n",
      "  f1_score = \t 0.9643353404353867 \n",
      "  AUC = \t 0.9924842885680466 \n",
      "  aupr = \t 0.9938399800499242 \n",
      "\n",
      "Round 5 cross validation\n",
      "\n",
      "the 5 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:46:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 5 th individual learner proformance: \n",
      "  Acc = \t 0.9613259668508287 \n",
      "  prec = \t 0.9604779411764706 \n",
      "  recall = \t 0.9622467771639043 \n",
      "  f1_score = \t 0.9613615455381785 \n",
      "  AUC = \t 0.9915940023537472 \n",
      "  aupr = \t 0.9931618454243665 \n",
      "\n",
      "Round 5 cross validation\n",
      "\n",
      "the 6 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:46:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 6 th individual learner proformance: \n",
      "  Acc = \t 0.9631675874769797 \n",
      "  prec = \t 0.9666048237476809 \n",
      "  recall = \t 0.9594843462246777 \n",
      "  f1_score = \t 0.9630314232902033 \n",
      "  AUC = \t 0.9922146590288587 \n",
      "  aupr = \t 0.9936924405296705 \n",
      "\n",
      "Round 5 cross validation\n",
      "\n",
      "the 7 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:46:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 7 th individual learner proformance: \n",
      "  Acc = \t 0.9594843462246777 \n",
      "  prec = \t 0.9637546468401487 \n",
      "  recall = \t 0.9548802946593001 \n",
      "  f1_score = \t 0.9592969472710453 \n",
      "  AUC = \t 0.9912387357596599 \n",
      "  aupr = \t 0.9928544798180818 \n",
      "\n",
      "Round 5 cross validation\n",
      "\n",
      "the 8 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:47:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 8 th individual learner proformance: \n",
      "  Acc = \t 0.9631675874769797 \n",
      "  prec = \t 0.9674721189591078 \n",
      "  recall = \t 0.9585635359116023 \n",
      "  f1_score = \t 0.9629972247918595 \n",
      "  AUC = \t 0.9917237297735451 \n",
      "  aupr = \t 0.9933202873931489 \n",
      "\n",
      "Round 5 cross validation\n",
      "\n",
      "the 9 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:47:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 9 th individual learner proformance: \n",
      "  Acc = \t 0.9567219152854513 \n",
      "  prec = \t 0.9635514018691589 \n",
      "  recall = \t 0.9493554327808471 \n",
      "  f1_score = \t 0.9564007421150279 \n",
      "  AUC = \t 0.9920120129286516 \n",
      "  aupr = \t 0.9933730505030618 \n",
      "\n",
      "Round 5 cross validation\n",
      "\n",
      "the 10 th individual learner training\n",
      "\n",
      "Feature selection in progress\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:47:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "the 10 th individual learner proformance: \n",
      "  Acc = \t 0.9571823204419889 \n",
      "  prec = \t 0.9592969472710453 \n",
      "  recall = \t 0.9548802946593001 \n",
      "  f1_score = \t 0.9570835256114444 \n",
      "  AUC = \t 0.9918805897255882 \n",
      "  aupr = \t 0.9933289250616584 \n",
      "\n",
      "\n",
      "Soft voting ...\n",
      "\n",
      "Round  4 cross validation proformance after soft voting: \n",
      "  Acc = \t 0.9654696132596685 \n",
      "  prec = \t 0.9693593314763231 \n",
      "  recall = \t 0.9613259668508287 \n",
      "  f1_score = \t 0.9653259361997226 \n",
      "  AUC = \t 0.9930099813802997 \n",
      "  AUPR = \t 0.9944126614932424 \n",
      "\n",
      "CSMDA Final proformance: \n",
      "  Acc = \t 0.961049723756906 \n",
      "  prec = \t 0.9680462618030994 \n",
      "  recall = \t 0.9535911602209947 \n",
      "  f1_score = \t 0.9607477063286449 \n",
      "  AUC = \t 0.9929716566785032 \n",
      "  AUPR = \t 0.9941479810242638\n"
     ]
    }
   ],
   "source": [
    "fold_num=5\n",
    "sel_fea = 'RF'\n",
    "prop = 0.75\n",
    "base_learner_num = 10\n",
    "\n",
    "#Hyper-parameters of Stochastic Forest Model\n",
    "sel_fea_num = int(prop*len(all_list[0]))\n",
    "sel_hp1 = 300\n",
    "sel_hp2 = 30\n",
    "\n",
    "posi_data = posi_list\n",
    "nega_data = new_nega\n",
    "posi_num = len(posi_data)\n",
    "nega_train_data, nega_cv_data = spliting_negative_data(nega_data, posi_num)\n",
    "\n",
    "soft_acc_list = []\n",
    "soft_prec_list = []\n",
    "soft_recall_list = []\n",
    "soft_f1_score_list = []\n",
    "soft_auc_list = []\n",
    "soft_aupr_list = []\n",
    "\n",
    "for fold in range(fold_num):\n",
    "    #Positive training sample\n",
    "    posi_train_data = np.array([x for i, x in enumerate(posi_data) if i % fold_num != fold])\n",
    "    #Positive test sample\n",
    "    posi_test_data = np.array([x for i, x in enumerate(posi_data) if i % fold_num == fold])\n",
    "    #Positive test sample negative\n",
    "    nega_test_data = np.array([x for i, x in enumerate(nega_cv_data) if i % fold_num == fold])\n",
    "    #Generate test set\n",
    "    X_test = np.concatenate((posi_test_data, nega_test_data))\n",
    "    y_test = np.concatenate((np.ones(posi_test_data.shape[0]), np.zeros(nega_test_data.shape[0])))\n",
    "    #Generate base_learner_num individual learners\n",
    "    base_learners = base_xgb_learners(base_learner_num)\n",
    "    trained_clfs = []\n",
    "    most_imps_list = []\n",
    "    for i in range(base_learner_num):\n",
    "        print('Round', fold+1, 'cross validation\\n')\n",
    "        print('the', i+1, 'th individual learner training\\n')\n",
    "        sample_num = posi_train_data.shape[0]\n",
    "        samples = random.sample(nega_train_data, sample_num)\n",
    "        samples = np.array(samples)\n",
    "        X_base_train = np.concatenate((posi_train_data, samples))\n",
    "        y_base_train = np.concatenate((np.ones(posi_train_data.shape[0]), np.zeros(samples.shape[0])))\n",
    "        if sel_fea == 'RF':\n",
    "            print('Feature selection in progress\\n')\n",
    "            most_imps = feature_ranking_by_rf(X_base_train, y_base_train, sel_fea_num, sel_hp1, sel_hp2)\n",
    "            most_imps_list.append(most_imps)\n",
    "            X_base_train = X_base_train[:,most_imps]\n",
    "            X_base_test = X_test[:,most_imps]\n",
    "        else:\n",
    "            X_base_train = X_base_train\n",
    "            X_base_test = X_test\n",
    "        clf = base_learners[i]\n",
    "        clf.fit(X_base_train,y_base_train)\n",
    "        trained_clfs.append(clf)\n",
    "        y_pred = clf.predict(X_base_test)\n",
    "        y_prob = clf.predict_proba(X_base_test)\n",
    "        y_prob = y_prob[:,1]\n",
    "        tp, fp, tn, fn, acc, prec, recall, f1_score, auroc, aupr = calculate_performace(len(y_pred), y_pred, y_prob, y_test) \n",
    "        print('the', i+1, 'th individual learner proformance: \\n  Acc = \\t', acc, \n",
    "              '\\n  prec = \\t', prec, \n",
    "              '\\n  recall = \\t', recall, \n",
    "              '\\n  f1_score = \\t', f1_score, \n",
    "              '\\n  AUC = \\t', auroc, \n",
    "              '\\n  aupr = \\t', aupr, \n",
    "              '\\n')\n",
    "        \n",
    "    if sel_fea == 'RF':\n",
    "        base_probs = base_probs_fs(X_test, most_imps_list, trained_clfs)\n",
    "    else:\n",
    "        base_probs = base_probs(X_test, trained_clfs)\n",
    "    #soft voting strategy\n",
    "    pred_final, prob_final = soft_voting_strategy(base_probs)\n",
    "    soft_tp, soft_fp, soft_tn, soft_fn, soft_acc, soft_prec, soft_recall, soft_f1_score, soft_auc, soft_aupr = calculate_performace(len(pred_final), pred_final, prob_final, y_test)\n",
    "    print('Round ', fold, 'cross validation proformance after soft voting: \\n  Acc = \\t', soft_acc, \n",
    "          '\\n  prec = \\t', soft_prec, \n",
    "          '\\n  recall = \\t', soft_recall, \n",
    "          '\\n  f1_score = \\t', soft_f1_score, \n",
    "          '\\n  AUC = \\t', soft_auc, \n",
    "          '\\n  AUPR = \\t', soft_aupr,\n",
    "          '\\n')\n",
    "    soft_acc_list.append(soft_acc)\n",
    "    soft_prec_list.append(soft_prec)\n",
    "    soft_recall_list.append(soft_recall)\n",
    "    soft_f1_score_list.append(soft_f1_score)\n",
    "    soft_auc_list.append(soft_auc)\n",
    "    soft_aupr_list.append(soft_aupr)\n",
    "\n",
    "soft_acc_arr = np.array(soft_acc_list)\n",
    "soft_prec_arr = np.array(soft_prec_list)\n",
    "soft_recall_arr = np.array(soft_recall_list)\n",
    "soft_f1_score_arr = np.array(soft_f1_score_list)\n",
    "soft_auc_arr = np.array(soft_auc_list)\n",
    "soft_aupr_arr = np.array(soft_aupr_list)\n",
    "        \n",
    "soft_ave_acc = np.mean(soft_acc_arr)\n",
    "soft_ave_prec = np.mean(soft_prec_arr)\n",
    "soft_ave_recall = np.mean(soft_recall_arr)\n",
    "soft_ave_f1_score = np.mean(soft_f1_score_arr)\n",
    "soft_ave_auc = np.mean(soft_auc_arr)\n",
    "soft_ave_aupr = np.mean(soft_aupr_arr)\n",
    "\n",
    "soft_std_acc = np.std(soft_acc_arr)\n",
    "soft_std_prec = np.std(soft_prec_arr)\n",
    "soft_std_recall = np.std(soft_recall_arr)\n",
    "soft_std_f1_score = np.std(soft_f1_score_arr)\n",
    "soft_std_auc = np.std(soft_auc_arr)\n",
    "soft_std_aupr = np.std(soft_aupr_arr)\n",
    "\n",
    "print('CSMDA Final proformance: \\n  Acc = \\t', soft_ave_acc, \n",
    "      '\\n  prec = \\t', soft_ave_prec, \n",
    "      '\\n  recall = \\t', soft_ave_recall, \n",
    "      '\\n  f1_score = \\t', soft_ave_f1_score, \n",
    "      '\\n  AUC = \\t', soft_ave_auc, \n",
    "      '\\n  AUPR = \\t', soft_ave_aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05846cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc = \t 0.961 \n",
      "  prec = \t 0.968 \n",
      "  recall = \t 0.9536 \n",
      "  f1_score = \t 0.9607 \n",
      "  AUC = \t 0.993 \n",
      "  aupr = \t 0.9941\n"
     ]
    }
   ],
   "source": [
    "soft_ave_acc=np.round(soft_ave_acc,4)\n",
    "soft_ave_prec=np.round(soft_ave_prec,4)\n",
    "soft_ave_recall=np.round(soft_ave_recall,4)\n",
    "soft_ave_f1_score=np.round(soft_ave_f1_score,4)\n",
    "soft_ave_auc=np.round(soft_ave_auc,4)\n",
    "soft_ave_aupr=np.round(soft_ave_aupr,4)\n",
    "print('  Acc = \\t', soft_ave_acc, \n",
    "      '\\n  prec = \\t', soft_ave_prec, \n",
    "      '\\n  recall = \\t', soft_ave_recall, \n",
    "      '\\n  f1_score = \\t', soft_ave_f1_score, \n",
    "      '\\n  AUC = \\t', soft_ave_auc, \n",
    "      '\\n  aupr = \\t', soft_ave_aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3bcc3426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc standard deviation= \t 0.0027 \n",
      "  prec standard deviation= \t 0.0025 \n",
      "  recall standard deviation= \t 0.0064 \n",
      "  f1_score standard deviation= \t 0.0029 \n",
      "  AUC standard deviation= \t 0.0009 \n",
      "  aupr standard deviation= \t 0.0007\n"
     ]
    }
   ],
   "source": [
    "soft_std_acc = np.round(soft_std_acc,4)\n",
    "soft_std_prec = np.round(soft_std_prec,4)\n",
    "soft_std_recall = np.round(soft_std_recall,4)\n",
    "soft_std_f1_score = np.round(soft_std_f1_score,4)\n",
    "soft_std_auc = np.round(soft_std_auc,4)\n",
    "soft_std_aupr = np.round(soft_std_aupr,4)\n",
    "print('  Acc standard deviation= \\t', soft_std_acc, \n",
    "      '\\n  prec standard deviation= \\t', soft_std_prec, \n",
    "      '\\n  recall standard deviation= \\t', soft_std_recall, \n",
    "      '\\n  f1_score standard deviation= \\t', soft_std_f1_score, \n",
    "      '\\n  AUC standard deviation= \\t', soft_std_auc, \n",
    "      '\\n  aupr standard deviation= \\t', soft_std_aupr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
